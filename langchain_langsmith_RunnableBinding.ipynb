{"cells":[{"cell_type":"code","source":["pip install langchain python-dotenv langchain_google_genai langchain_community langchain-core bs4 faiss-cpu transformers sentence-transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Io_38hXIy74V","executionInfo":{"status":"ok","timestamp":1739408153248,"user_tz":-330,"elapsed":118725,"user":{"displayName":"Sarvesh Havalagi","userId":"01205626820814641348"}},"outputId":"6bcd062e-410c-40e8-8719-6391cc4e6832"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.17)\n","Collecting python-dotenv\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Collecting langchain_google_genai\n","  Downloading langchain_google_genai-2.0.9-py3-none-any.whl.metadata (3.6 kB)\n","Collecting langchain_community\n","  Downloading langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.33)\n","Collecting bs4\n","  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n","Collecting faiss-cpu\n","  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.2)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.12)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n","Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n","Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.8.4)\n","Collecting langchain-core\n","  Downloading langchain_core-0.3.35-py3-none-any.whl.metadata (5.9 kB)\n","Collecting langchain\n","  Downloading langchain-0.3.18-py3-none-any.whl.metadata (7.8 kB)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n","  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n","Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain)\n","  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.12.2)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.19.2)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.160.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.27.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.25.6)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.26.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.6)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.66.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (5.5.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.9)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.22.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.2.0)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.1.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.70.0)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.62.3)\n","Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.2.1)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.1)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n","Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading langchain_google_genai-2.0.9-py3-none-any.whl (41 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_community-0.3.17-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain-0.3.18-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.35-py3-none-any.whl (413 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.2/413.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n","Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n","Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: filetype, python-dotenv, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, nvidia-cusparse-cu12, nvidia-cudnn-cu12, bs4, pydantic-settings, nvidia-cusolver-cu12, dataclasses-json, langchain-core, langchain-text-splitters, langchain, langchain_google_genai, langchain_community\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 0.3.33\n","    Uninstalling langchain-core-0.3.33:\n","      Successfully uninstalled langchain-core-0.3.33\n","  Attempting uninstall: langchain-text-splitters\n","    Found existing installation: langchain-text-splitters 0.3.5\n","    Uninstalling langchain-text-splitters-0.3.5:\n","      Successfully uninstalled langchain-text-splitters-0.3.5\n","  Attempting uninstall: langchain\n","    Found existing installation: langchain 0.3.17\n","    Uninstalling langchain-0.3.17:\n","      Successfully uninstalled langchain-0.3.17\n","Successfully installed bs4-0.0.2 dataclasses-json-0.6.7 faiss-cpu-1.10.0 filetype-1.2.0 httpx-sse-0.4.0 langchain-0.3.18 langchain-core-0.3.35 langchain-text-splitters-0.3.6 langchain_community-0.3.17 langchain_google_genai-2.0.9 marshmallow-3.26.1 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vvTsn3gSyR25","executionInfo":{"status":"ok","timestamp":1739408245444,"user_tz":-330,"elapsed":402,"user":{"displayName":"Sarvesh Havalagi","userId":"01205626820814641348"}},"outputId":"5b94d9f7-462a-4c8f-c9ac-b573df9fe54e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":3}],"source":["import os\n","from dotenv import load_dotenv\n","load_dotenv()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"id":"j96U9sKXyR3B","executionInfo":{"status":"error","timestamp":1739408253240,"user_tz":-330,"elapsed":388,"user":{"displayName":"Sarvesh Havalagi","userId":"01205626820814641348"}},"outputId":"5a95156c-4e00-42a4-ddec-e099605f3d46"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"str expected, not NoneType","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-d41c1dea4a66>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"GOOGLEGEMINI_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GOOGLEGEMINI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Langsmith Tracking and tracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# LANGCHAIN_API_KEY: API key for LangChain services, such as LangSmith (a platform for debugging,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(value)\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: str expected, not NoneType"]}],"source":["os.environ[\"GOOGLEGEMINI_API_KEY\"] = os.getenv(\"GOOGLEGEMINI_API_KEY\")\n","\n","# Langsmith Tracking and tracing\n","\n","# LANGCHAIN_API_KEY: API key for LangChain services, such as LangSmith (a platform for debugging,\n","# testing, and monitoring LLM applications).\n","\n","# LANGCHAIN_TRACING_V2: Enables tracing for your LangChain application. Tracing logs the execution\n","# of your LLM calls, chains, and agents, which helps you debug and analyze the behavior of your application.\n","\n","os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n","os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n","os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uDas4xEKyR3C"},"outputs":[],"source":["'''\n","How LangSmith Helps in Debugging, Testing, and Monitoring\n","\n","Debugging:\n","When you run the code, LangSmith logs the following:\n","The input to the LLM (\"What is agentic AI\").\n","The output from the LLM.\n","The prompt template used.\n","Any intermediate steps (if you were using chains or agents).\n","You can view these logs in LangSmith to understand how the LLM processed the input and generated the output. If the output is incorrect or unexpected, you can trace back to identify the issue.\n","\n","Testing:\n","You can test your application with different inputs and analyze the results in LangSmith. For example:\n","Test edge cases or unusual inputs to see how the LLM responds.\n","Compare the outputs of different models or configurations.\n","LangSmith provides a detailed record of each test run, making it easy to identify and fix issues.\n","\n","Monitoring:\n","In a production environment, LangSmith can monitor your LLM application in real time. For example:\n","Track the performance of your LLM calls (e.g., latency, success rate).\n","Detect anomalies or errors in the application.\n","Analyze usage patterns and optimize resource allocation.\n","\n","Tracing:\n","Tracing provides a detailed record of the execution of your LLM calls, chains, and agents. For example:\n","You can see how the input flows through your application.\n","You can identify bottlenecks or inefficiencies in your chains or agents.\n","You can analyze the behavior of your application over time.\n","'''"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g3aN4WQYyR3D","executionInfo":{"status":"ok","timestamp":1739408342716,"user_tz":-330,"elapsed":3409,"user":{"displayName":"Sarvesh Havalagi","userId":"01205626820814641348"}},"outputId":"28bb6716-2c4c-409d-8bc1-8e3708f0e180"},"outputs":[{"output_type":"stream","name":"stdout","text":["model='models/gemini-2.0-flash' google_api_key=SecretStr('**********') client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7e88b5618350> default_metadata=()\n"]}],"source":["import os\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","# Set API Key (Replace with your actual API key)\n","os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyA5_-DBvzlFOh1cmE6T8pzWroxIMAWa0UQ\"\n","\n","# Initialize Gemini Model\n","llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n","\n","print(llm)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RIIaXoBTyR3F","executionInfo":{"status":"ok","timestamp":1739408377120,"user_tz":-330,"elapsed":11394,"user":{"displayName":"Sarvesh Havalagi","userId":"01205626820814641348"}},"outputId":"cd536c89-8c54-49b7-8246-31f294a59e06"},"outputs":[{"output_type":"stream","name":"stdout","text":["content=\"Agentic AI, also known as autonomous AI or AI agents, refers to AI systems that can **perceive their environment, make decisions, take actions, and learn from the results to achieve a specific goal without explicit human instruction at every step.** They are designed to be proactive, self-improving, and capable of adapting to changing circumstances.\\n\\nHere's a breakdown of key aspects of Agentic AI:\\n\\n*   **Autonomy:** This is the core characteristic. Agents operate with minimal human intervention, making decisions and executing actions independently.\\n*   **Perception:** They gather information from their environment through sensors, APIs, or other data sources. This could be anything from reading text to analyzing images to monitoring financial markets.\\n*   **Decision-Making:** Based on their perception and goals, agents use algorithms (often involving machine learning, planning, and reasoning) to decide what actions to take.\\n*   **Action:** They execute their decisions by interacting with their environment, such as sending emails, controlling robots, or making trades.\\n*   **Learning:** Agents learn from their experiences, improving their performance over time through feedback loops and reinforcement learning.  They can adjust their strategies and adapt to new situations.\\n*   **Goal-Oriented:** They are designed to achieve specific objectives, which can range from simple tasks to complex, long-term goals.\\n*   **Persistence:**  Agentic AI often maintains state and memory of past interactions and experiences, enabling them to make more informed decisions over time.\\n\\n**Key Components Often Found in Agentic AI Systems:**\\n\\n*   **Planning Module:**  Formulates strategies and sequences of actions to achieve the goal.\\n*   **Memory/Knowledge Base:** Stores information about the environment, past experiences, and learned knowledge.\\n*   **Reasoning Engine:**  Uses logic and inference to draw conclusions and make predictions.\\n*   **Action Execution Module:**  Carries out the chosen actions.\\n*   **Learning Module:**  Updates the agent's knowledge and decision-making processes based on feedback.\\n\\n**Examples of Agentic AI in Development and Use:**\\n\\n*   **Autonomous Vehicles:**  Perceive the road, make decisions about steering and speed, and navigate to a destination without human control.\\n*   **Robotics:** Robots that can perform tasks in complex environments, such as cleaning, assembly, or exploration, adapting to unforeseen obstacles.\\n*   **Personal Assistants:** More advanced assistants that can proactively manage schedules, make recommendations, and automate tasks based on user preferences and context.\\n*   **Financial Trading Bots:**  Analyze market data, make trading decisions, and execute trades automatically to maximize profits.\\n*   **Cybersecurity Systems:** Detect and respond to threats in real-time, adapting to evolving attack patterns.\\n*   **Scientific Discovery:**  Automated experimentation and analysis to accelerate scientific research.\\n*   **Code Generation:** AI agents that can autonomously write and debug code based on high-level instructions.\\n\\n**Distinction from Traditional AI:**\\n\\nTraditional AI systems are typically designed for specific, well-defined tasks and require explicit instructions for each step. Agentic AI, on the other hand, is more flexible and can handle more complex and dynamic situations with less human intervention.\\n\\n**Challenges and Considerations:**\\n\\n*   **Safety and Control:**  Ensuring that autonomous agents act in a safe and ethical manner, and preventing unintended consequences.\\n*   **Explainability and Transparency:** Understanding how agents make decisions and providing explanations for their actions.\\n*   **Bias and Fairness:**  Addressing potential biases in the data and algorithms used by agents.\\n*   **Robustness:**  Ensuring that agents can function reliably in unpredictable environments.\\n*   **Trust:** Building trust in autonomous systems, especially in critical applications.\\n\\n**In summary, Agentic AI represents a significant step towards more intelligent and autonomous systems that can solve complex problems and interact with the world in a more proactive and adaptive way.** It's a rapidly evolving field with the potential to transform many aspects of our lives.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-6fe449f0-b465-47d1-a023-f30ee4bde8e8-0' usage_metadata={'input_tokens': 5, 'output_tokens': 830, 'total_tokens': 835, 'input_token_details': {'cache_read': 0}}\n"]}],"source":["# This question and answer will be stored in 'Tracing projects' section in smith.langchain.com (this we will get on logging in into langchain.com\n","# website)\n","result=llm.invoke(\"What is agentic AI\")\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yROBGJZVyR3G","outputId":"70905f52-151e-4e1c-a722-8d032abad33a"},"outputs":[{"data":{"text/plain":["ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# System prompt is the instruction given to the llm model.\n","# User prompt is the instructuin given by the user\n","# this template can be obtained from https://python.langchain.com/docs/concepts/prompt_templates/ website\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","prompt=ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n","        (\"user\",\"{input}\")\n","\n","    ]\n",")\n","prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_WEnhrLhyR3H","outputId":"9953492f-adfa-4c85-aa99-958d3527adbe"},"outputs":[{"name":"stdout","output_type":"stream","text":["content=\"Okay, let's dive into LangSmith. I can give you a comprehensive overview of what it is, its purpose, key features, and how it fits into the broader landscape of LLM development.\\n\\n**What is LangSmith?**\\n\\nLangSmith is a platform developed by LangChain (the company behind the popular LangChain framework) that provides a unified environment for debugging, testing, evaluating, and monitoring your LLM-powered applications. Think of it as a set of developer tools specifically designed for working with large language models (LLMs) and the complex chains and agents built around them.\\n\\n**In essence, LangSmith helps you:**\\n\\n*   **Understand:** Gain deep visibility into how your LLM applications are behaving.\\n*   **Debug:** Identify and fix issues in your prompts, chains, and agents.\\n*   **Evaluate:** Systematically measure the quality and performance of your applications.\\n*   **Monitor:** Track the health and performance of your applications in production.\\n\\n**Why is LangSmith Important?**\\n\\nLLM applications are notoriously difficult to debug and evaluate compared to traditional software. Here's why:\\n\\n*   **Non-Deterministic Behavior:** LLMs can produce different outputs for the same input, making it hard to reproduce errors.\\n*   **Complex Chains:** Applications often involve multiple LLM calls chained together with other components, making it hard to isolate the source of a problem.\\n*   **Subjective Evaluation:** Evaluating the quality of LLM outputs can be subjective and require human judgment.\\n*   **Evolving Models:** LLMs are constantly being updated, which can impact the performance of your applications.\\n\\nLangSmith addresses these challenges by providing the tools and infrastructure needed to develop, test, and deploy LLM applications with confidence.\\n\\n**Key Features of LangSmith:**\\n\\n*   **Tracing:**\\n    *   **Detailed Execution Traces:** Captures every step in the execution of your chains and agents, including inputs, outputs, intermediate steps, and latency.\\n    *   **Visualization:** Presents traces in a clear, interactive interface, allowing you to drill down into specific parts of the execution.\\n    *   **Integration with LangChain:** Seamlessly integrates with LangChain, automatically logging traces for your LangChain applications.  You can also manually log traces using the LangSmith SDK for other LLM frameworks.\\n*   **Evaluation:**\\n    *   **Test Datasets:** Create and manage datasets of inputs and expected outputs for your applications.\\n    *   **Automated Evaluation:** Run your applications against test datasets and automatically evaluate the results using metrics like accuracy, relevance, and coherence.\\n    *   **Human-in-the-Loop Evaluation:** Incorporate human feedback into the evaluation process to capture subjective aspects of quality.\\n    *   **Customizable Metrics:** Define your own evaluation metrics tailored to your specific application.\\n*   **Testing:**\\n    *   **Unit Testing for LLMs:** Allows you to write unit tests for individual components of your LLM applications, such as prompts or chains.\\n    *   **Integration Testing:** Test the integration of multiple components together.\\n    *   **Regression Testing:** Detect regressions in performance when you update your models or code.\\n*   **Monitoring:**\\n    *   **Real-time Monitoring:** Track the performance of your applications in production, including latency, error rates, and token usage.\\n    *   **Alerting:** Set up alerts to notify you when performance degrades or errors occur.\\n    *   **Data Analysis:** Analyze your data to identify trends and patterns in your application's behavior.\\n*   **Hub:**\\n    *   **Centralized Registry:** A place to share, discover, and reuse prompts, chains, and agents.\\n    *   **Version Control:** Track changes to your prompts, chains, and agents over time.\\n    *   **Collaboration:** Collaborate with other developers on LLM projects.\\n*   **Data Management:**\\n    *   **Store and organize your LLM application data:** Including prompts, responses, datasets, and evaluation results.\\n    *   **Data versioning:** Track changes to your data over time.\\n\\n**How LangSmith Works (Simplified Workflow):**\\n\\n1.  **Instrument Your Code:** Integrate the LangSmith SDK into your LLM application code.  Typically, this involves setting environment variables.\\n2.  **Run Your Application:** As your application runs, LangSmith automatically captures traces of the execution.\\n3.  **Explore Traces:** Use the LangSmith UI to explore the traces, identify issues, and understand how your application is behaving.\\n4.  **Create Test Datasets:** Create datasets of inputs and expected outputs for your application.\\n5.  **Evaluate Your Application:** Run your application against the test datasets and evaluate the results.\\n6.  **Monitor in Production:** Deploy your application to production and monitor its performance using LangSmith.\\n\\n**Benefits of Using LangSmith:**\\n\\n*   **Faster Debugging:** Quickly identify and fix issues in your LLM applications.\\n*   **Improved Quality:** Systematically evaluate and improve the quality of your applications.\\n*   **Increased Confidence:** Deploy your applications to production with confidence.\\n*   **Reduced Costs:** Optimize your applications to reduce token usage and costs.\\n*   **Better Collaboration:** Collaborate more effectively with other developers on LLM projects.\\n\\n**Who Should Use LangSmith?**\\n\\nLangSmith is valuable for:\\n\\n*   **LLM Application Developers:** Anyone building applications that use large language models.\\n*   **Data Scientists:** Data scientists who are experimenting with and fine-tuning LLMs.\\n*   **MLOps Engineers:** MLOps engineers who are responsible for deploying and monitoring LLM applications in production.\\n*   **AI Product Managers:** Product managers who need to understand the performance and quality of their LLM-powered products.\\n\\n**LangSmith vs. Alternatives:**\\n\\nWhile other tools offer some overlapping functionalities, LangSmith distinguishes itself with its deep integration with LangChain and its focus on the entire LLM application development lifecycle. Some alternatives or complementary tools include:\\n\\n*   **OpenAI Evals:** Focuses primarily on evaluating LLM outputs.\\n*   **Weights & Biases:** A broader MLOps platform that can be used to track and visualize LLM experiments.\\n*   **Phoenix:** Observability and evaluation tool for LLMs and vector databases, part of Arize AI.\\n*   **Custom Logging and Monitoring:** Building your own logging and monitoring infrastructure (which can be time-consuming and complex).\\n\\n**Pricing:**\\n\\nLangSmith offers different pricing tiers, including a free tier for small projects and paid tiers for larger projects with higher usage limits.  Check the LangChain website for the most up-to-date pricing information.\\n\\n**In Summary:**\\n\\nLangSmith is a powerful platform that provides essential tools for developing, testing, evaluating, and monitoring LLM applications. Its deep integration with LangChain, comprehensive feature set, and focus on the entire LLM application lifecycle make it a valuable asset for any team building with LLMs.  By providing observability and structure to the inherently messy world of LLMs, LangSmith helps developers build more reliable, efficient, and effective AI-powered applications.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-74b36982-b365-4bfa-af3f-bd9e2180c500-0' usage_metadata={'input_tokens': 21, 'output_tokens': 1502, 'total_tokens': 1523, 'input_token_details': {'cache_read': 0}}\n"]}],"source":["# The input given by the user should be forst passed to prompt and then to llm model so chain is created\n","chain=prompt|llm\n","\n","response=chain.invoke({\"input\":\"Can you tell me about Langsmith\"})\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48rNf3atyR3I","outputId":"842be50d-8253-409d-bf0e-02c3dc3a4ab0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Okay, let's delve into LangSmith. I'll give you a comprehensive overview, covering its purpose, key features, benefits, and how it fits into the broader LangChain ecosystem.\n","\n","**What is LangSmith?**\n","\n","LangSmith is a unified platform designed to help you **debug, test, evaluate, and monitor** your LangChain applications (LLM-powered applications).  It's essentially a developer toolset specifically built to address the unique challenges of working with LLMs, which can be unpredictable and difficult to debug compared to traditional software. Think of it as the \"developer tools\" for your LLM-powered apps.\n","\n","**Why is LangSmith Important?**\n","\n","LLM applications are notoriously difficult to debug and evaluate due to the following reasons:\n","\n","*   **Non-Deterministic Output:**  LLMs don't always give the same answer for the same input.\n","*   **Complex Chains:**  LLM apps often involve multiple steps, making it hard to pinpoint where things go wrong.\n","*   **Lack of Observability:**  Without proper tools, it's difficult to see what's happening inside your LLM application as it runs.\n","*   **Subjective Evaluation:**  Evaluating the \"correctness\" of an LLM's response can be subjective and require human judgment.\n","\n","LangSmith addresses these challenges by providing the necessary tools and infrastructure to make LLM application development more manageable and reliable.\n","\n","**Key Features and Components:**\n","\n","1.  **Tracing:**\n","\n","    *   **Purpose:** Captures detailed execution traces of your LangChain application.  This is like a detailed log or timeline of every step, from the initial input to the final output.\n","    *   **Functionality:**\n","        *   Records inputs, outputs, intermediate steps, and durations for each component in your LangChain chain or agent.\n","        *   Visualizes the flow of data through your application.\n","        *   Allows you to inspect the exact prompts sent to the LLM and the responses received.\n","        *   Supports filtering and searching through traces to find specific issues.\n","    *   **Benefit:**  Enables you to understand exactly how your application is behaving, identify bottlenecks, and pinpoint the source of errors.\n","\n","2.  **Evaluation:**\n","\n","    *   **Purpose:**  Provides tools to systematically evaluate the performance of your LLM application.\n","    *   **Functionality:**\n","        *   Define evaluation criteria (e.g., accuracy, relevance, coherence, helpfulness).\n","        *   Create datasets of input/output examples.\n","        *   Run your application on the dataset and automatically evaluate the results.\n","        *   Supports different evaluation methods:\n","            *   **LLM-based Evaluation:**  Using LLMs themselves to evaluate the quality of the responses.\n","            *   **Human Evaluation:**  Involving human reviewers to provide feedback.\n","            *   **Metric-based Evaluation:**  Using predefined metrics to assess performance.\n","        *   Provides dashboards and reports to visualize evaluation results.\n","    *   **Benefit:**  Helps you quantify the performance of your application, identify areas for improvement, and track progress over time.  It also helps you compare different versions of your application or different LLM models.\n","\n","3.  **Testing:**\n","\n","    *   **Purpose:**  Allows you to create unit tests and integration tests for your LangChain components and applications.\n","    *   **Functionality:**\n","        *   Define test cases with specific inputs and expected outputs.\n","        *   Run tests automatically and check if the actual outputs match the expected outputs.\n","        *   Supports mocking and patching to isolate components during testing.\n","    *   **Benefit:**  Ensures that your application behaves as expected, prevents regressions, and makes it easier to refactor your code.\n","\n","4.  **Monitoring:**\n","\n","    *   **Purpose:**  Provides real-time monitoring of your LangChain application in production.\n","    *   **Functionality:**\n","        *   Tracks key metrics such as latency, error rate, and token usage.\n","        *   Alerts you to potential issues or performance degradations.\n","        *   Allows you to drill down into individual traces to investigate problems.\n","    *   **Benefit:**  Helps you proactively identify and resolve issues before they impact users, and optimize your application for performance and cost.\n","\n","5.  **Datasets:**\n","\n","    *   **Purpose:**  Allows you to create, manage, and version datasets for evaluation and testing.\n","    *   **Functionality:**\n","        *   Import data from various sources (e.g., CSV files, databases, APIs).\n","        *   Annotate data with labels and metadata.\n","        *   Split datasets into training, validation, and test sets.\n","        *   Version control datasets to track changes over time.\n","    *   **Benefit:**  Ensures that your evaluation and testing are based on high-quality, representative data.  It also makes it easier to reproduce results and compare different versions of your application.\n","\n","6.  **Hub Integration:**\n","\n","    *   **Purpose:** Integrates with the LangChain Hub to share and discover prompts, chains, and agents.\n","    *   **Functionality:**\n","        *   Share your own prompts, chains, and agents with the community.\n","        *   Discover and use prompts, chains, and agents created by others.\n","        *   Version control and track the usage of shared components.\n","    *   **Benefit:**  Facilitates collaboration and knowledge sharing within the LangChain community.\n","\n","**How LangSmith Works (Simplified Workflow):**\n","\n","1.  **Instrument your LangChain application:**  You add LangSmith tracing to your code (usually with a few lines of code).\n","2.  **Run your application:**  As your application runs, LangSmith captures detailed traces of its execution.\n","3.  **View traces in the LangSmith UI:**  You can then use the LangSmith UI to explore the traces, identify issues, and debug your application.\n","4.  **Create datasets and evaluations:**  You can define evaluation criteria and create datasets to systematically evaluate the performance of your application.\n","5.  **Run evaluations and analyze results:**  LangSmith automatically evaluates your application on the dataset and provides you with detailed results.\n","6.  **Iterate and improve:**  Based on the evaluation results, you can iterate on your application, make changes, and re-evaluate to track progress.\n","\n","**Benefits of Using LangSmith:**\n","\n","*   **Improved Debugging:**  Pinpoint the root cause of issues in your LLM applications.\n","*   **Systematic Evaluation:**  Quantify the performance of your applications and track progress over time.\n","*   **Increased Reliability:**  Ensure that your applications behave as expected and prevent regressions.\n","*   **Faster Development:**  Accelerate the development cycle by providing the tools you need to quickly debug, test, and evaluate your applications.\n","*   **Better Collaboration:**  Share your prompts, chains, and agents with the community.\n","*   **Enhanced Observability:**  Gain real-time insights into the behavior of your applications in production.\n","*   **Cost Optimization:**  Identify and eliminate inefficiencies in your LLM application to reduce costs.\n","\n","**LangSmith and LangChain: A Symbiotic Relationship**\n","\n","LangSmith is deeply integrated with LangChain.  It's designed to work seamlessly with LangChain's core components (chains, agents, prompts, etc.).  LangChain provides the building blocks for creating LLM applications, while LangSmith provides the tools for developing, debugging, and deploying those applications.\n","\n","**Who Should Use LangSmith?**\n","\n","*   **Developers building LLM-powered applications:**  Especially those using LangChain.\n","*   **Data scientists and machine learning engineers:**  Who want to evaluate and improve the performance of their LLM models.\n","*   **Teams working on complex LLM applications:**  Where collaboration and observability are critical.\n","*   **Organizations deploying LLM applications to production:**  Who need to monitor performance and ensure reliability.\n","\n","**Getting Started with LangSmith:**\n","\n","1.  **Sign up for a LangSmith account:**  Visit the LangSmith website ([https://www.smith.langchain.com/](https://www.smith.langchain.com/)) and create an account.\n","2.  **Install the LangChain Python package:**  `pip install langchain`\n","3.  **Configure your environment variables:**  Set the necessary environment variables (e.g., `LANGCHAIN_TRACING_V2=true`, `LANGCHAIN_API_KEY=<your_api_key>`, `LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"`)\n","4.  **Instrument your LangChain application:**  Add LangSmith tracing to your code.  The LangChain documentation provides detailed instructions and examples.\n","5.  **Start exploring the LangSmith UI:**  Run your application and view the traces in the LangSmith UI.\n","\n","**Example (Basic Tracing):**\n","\n","```python\n","import os\n","from langchain.llms import OpenAI\n","from langchain.chains import LLMChain\n","from langchain.prompts import PromptTemplate\n","\n","os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n","os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR_API_KEY\"  # Replace with your actual API key\n","os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n","\n","# Define a prompt template\n","prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n","\n","# Create an LLM\n","llm = OpenAI(temperature=0.7)\n","\n","# Create an LLMChain\n","chain = LLMChain(llm=llm, prompt=prompt)\n","\n","# Run the chain\n","joke = chain.run(\"cats\")\n","\n","print(joke)\n","```\n","\n","When you run this code, LangSmith will capture a trace of the execution, including the prompt, the LLM's response, and the time taken. You can then view the trace in the LangSmith UI to see exactly what happened.\n","\n","**In Summary:**\n","\n","LangSmith is a powerful platform that significantly improves the development, debugging, evaluation, and monitoring of LangChain applications. By providing detailed tracing, evaluation tools, and monitoring capabilities, LangSmith helps developers build more reliable, performant, and cost-effective LLM applications. If you're working with LangChain, LangSmith is an essential tool to have in your arsenal.\n"]}],"source":["'''From the above output (content='Okay, let\\'s dive into LangSmith.....) we have to remove 'content=' since we are building chatbot.\n","An Output Parser in LangChain is used to convert raw LLM responses into structured formats like:\n","Strings (StrOutputParser)\n","JSON (JsonOutputParser)\n","Custom objects (Pydantic, Lists, Dicts, etc.)\n","\n","Why Use StrOutputParser?\n","Removes metadata from raw LLM responses\n","Ensures clean output for further processing\n","Converts response into a simple string'''\n","from langchain_core.output_parsers import StrOutputParser #StrOutputParser- to get string output\n","\n","from langchain_core.output_parsers import StrOutputParser\n","output_parser=StrOutputParser()\n","\n","chain=prompt|llm|output_parser\n","\n","response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OqvvIVDJyR3J","outputId":"368767dc-2e27-4fa5-fc8f-7e43fe402de9"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'summary': 'LangSmith is a platform developed by LangChain for debugging, testing, evaluating, and monitoring your LangChain applications and LLMs. It provides tools to trace and visualize the execution of your chains and agents, identify performance bottlenecks, and improve the overall reliability and quality of your language model applications.', 'key_features': [{'name': 'Tracing and Debugging', 'description': 'LangSmith allows you to trace the entire execution flow of your LangChain applications, including inputs, outputs, intermediate steps, and LLM calls. This helps you understand how your application is behaving and identify the source of any errors or unexpected results. You can inspect individual steps, view LLM prompts and responses, and see the data transformations that are happening at each stage.'}, {'name': 'Evaluation and Testing', 'description': 'LangSmith provides tools for evaluating the performance of your LangChain applications. You can define test cases, run your application against those test cases, and automatically evaluate the results. This helps you identify areas where your application can be improved and track progress over time. You can evaluate using custom criteria or built-in metrics like latency, accuracy, and cost.'}, {'name': 'Monitoring and Observability', 'description': 'LangSmith allows you to monitor the performance of your LangChain applications in production. You can track key metrics like latency, error rates, and cost, and set up alerts to be notified of any issues. This helps you ensure that your applications are running smoothly and that you are getting the most value from your LLMs.'}, {'name': 'Data Management', 'description': \"LangSmith allows you to store and manage your application's data, including inputs, outputs, and feedback. This data can be used to improve your application's performance, train new models, and debug issues. You can also use the data to track the performance of your application over time and identify trends.\"}, {'name': 'Collaboration', 'description': 'LangSmith facilitates collaboration among team members. You can share traces, evaluations, and datasets with your colleagues to help them understand your application and contribute to its improvement.  This shared context helps streamline debugging and iteration.'}], 'use_cases': ['Debugging complex LangChain applications', 'Evaluating the performance of different LLM models', 'Identifying performance bottlenecks in your application', 'Improving the reliability and quality of your language model applications', 'Tracking the performance of your application over time', 'Collaborating with team members on language model development'], 'integration': 'LangSmith integrates seamlessly with the LangChain framework. You can easily enable tracing and logging in your LangChain applications with a few lines of code.', 'relationship_to_langchain': \"LangSmith is developed by the same team behind LangChain and is designed to be the go-to platform for managing and improving LangChain applications.  It's essentially the observability and development platform *for* LangChain.\", 'access': 'LangSmith is offered as a paid service, though a free tier is available for smaller projects and exploration.', 'additional_resources': [{'name': 'LangSmith Documentation', 'url': 'https://docs.smith.langchain.com/'}, {'name': 'LangChain Website', 'url': 'https://www.langchain.com/'}]}\n"]}],"source":["# JsonOutputParser- to get output in the form of json\n","from langchain_core.prompts import PromptTemplate\n","from langchain_core.output_parsers import JsonOutputParser\n","\n","\n","output_parser=JsonOutputParser()\n","prompt = PromptTemplate(\n","    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n","    input_variables=[\"query\"],\n","    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",")\n","\n","chain=prompt|llm|output_parser\n","\n","response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_UW0ow-1yR3L"},"outputs":[],"source":["# Lets Build RAG"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZC9slAFyR3L","executionInfo":{"status":"ok","timestamp":1739408487395,"user_tz":-330,"elapsed":466,"user":{"displayName":"Sarvesh Havalagi","userId":"01205626820814641348"}},"outputId":"2f661c7a-6c54-4364-d869-a9b08c55479f"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"]}],"source":["# we have installed bs4 library, it helps in web scraping\n","# WebBaseLoader helps fetch and process web page content as documents. It retrieves web page content from a given URL. It parses the text\n","# content from the HTML of the web page. It returns the extracted content as a Document object, which can be used for further processing in\n","# LangChain applications.\n","# https://python.langchain.com/docs/integrations/document_loaders/web_base/\n","from langchain_community.document_loaders import WebBaseLoader"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ezrtbOwwyR3M","executionInfo":{"status":"ok","timestamp":1739408492073,"user_tz":-330,"elapsed":422,"user":{"displayName":"Sarvesh Havalagi","userId":"01205626820814641348"}},"outputId":"57307594-627e-4cec-e326-4d33200099a9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<langchain_community.document_loaders.web_base.WebBaseLoader at 0x7e88b449fb10>"]},"metadata":{},"execution_count":10}],"source":["loader=WebBaseLoader(\"https://python.langchain.com/docs/tutorials/llm_chain/\")\n","loader"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XGtP43_NyR3M","executionInfo":{"status":"ok","timestamp":1739408496219,"user_tz":-330,"elapsed":1245,"user":{"displayName":"Sarvesh Havalagi","userId":"01205626820814641348"}},"outputId":"de54d031-78a1-4d3b-ed09-aafcacfb7f3c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a simple LLM application with chat models and prompt templatesOn this pageBuild a simple LLM application with chat models and prompt templates\\nIn this quickstart we\\'ll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it\\'s just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\\nAfter reading this tutorial, you\\'ll have a high level overview of:\\n\\n\\nUsing language models\\n\\n\\nUsing prompt templates\\n\\n\\nDebugging and tracing your application using LangSmith\\n\\n\\nLet\\'s dive in!\\nSetup‚Äã\\nJupyter Notebook‚Äã\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\\nInstallation‚Äã\\nTo install LangChain run:\\n\\nPipCondapip install langchainconda install langchain -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith‚Äã\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nUsing Language Models‚Äã\\nFirst up, let\\'s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to supported integrations.\\n\\nSelect chat model:Groq‚ñæGroqOpenAIAnthropicAzureGoogle VertexAWSCohereNVIDIAFireworks AIMistral AITogether AIDatabrickspip install -qU \"langchain[groq]\"import getpassimport osif not os.environ.get(\"GROQ_API_KEY\"):  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\\nLet\\'s first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the .invoke method.\\nfrom langchain_core.messages import HumanMessage, SystemMessagemessages = [    SystemMessage(\"Translate the following from English into Italian\"),    HumanMessage(\"hi!\"),]model.invoke(messages)API Reference:HumanMessage | SystemMessage\\nAIMessage(content=\\'Ciao!\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 3, \\'prompt_tokens\\': 20, \\'total_tokens\\': 23, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-32654a56-627c-40e1-a141-ad9350bbfd3e-0\\', usage_metadata={\\'input_tokens\\': 20, \\'output_tokens\\': 3, \\'total_tokens\\': 23, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\ntipIf we\\'ve enabled LangSmith, we can see that this run is logged to LangSmith, and can see the LangSmith trace. The LangSmith trace reports token usage information, latency, standard model parameters (such as temperature), and other information.\\nNote that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke(\"Hello\")model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])model.invoke([HumanMessage(\"Hello\")])\\nStreaming‚Äã\\nBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates‚Äã\\nRight now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\\nPrompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\\nLet\\'s create a prompt template here. It will take in two user variables:\\n\\nlanguage: The language to translate text into\\ntext: The text to translate\\n\\nfrom langchain_core.prompts import ChatPromptTemplatesystem_template = \"Translate the following from English into {language}\"prompt_template = ChatPromptTemplate.from_messages(    [(\"system\", system_template), (\"user\", \"{text}\")])API Reference:ChatPromptTemplate\\nNote that ChatPromptTemplate supports multiple message roles in a single template. We format the language parameter into the system message, and the user text into a user message.\\nThe input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself\\nprompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})prompt\\nChatPromptValue(messages=[SystemMessage(content=\\'Translate the following from English into Italian\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'hi!\\', additional_kwargs={}, response_metadata={})])\\nWe can see that it returns a ChatPromptValue that consists of two messages. If we want to access the messages directly we do:\\nprompt.to_messages()\\n[SystemMessage(content=\\'Translate the following from English into Italian\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'hi!\\', additional_kwargs={}, response_metadata={})]\\nFinally, we can invoke the chat model on the formatted prompt:\\nresponse = model.invoke(prompt)print(response.content)\\nCiao!\\ntipMessage content can contain both text and content blocks with additional structure. See this guide for more information.\\nIf we take a look at the LangSmith trace, we can see exactly what prompt the chat model receives, along with token usage information, latency, standard model parameters (such as temperature), and other information.\\nConclusion‚Äã\\nThat\\'s it! In this tutorial you\\'ve learned how to create your first simple LLM application. You\\'ve learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith.\\nThis just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we\\'ve got a lot of other resources!\\nFor further reading on the core concepts of LangChain, we\\'ve got detailed Conceptual Guides.\\nIf you have more specific questions on these concepts, check out the following sections of the how-to guides:\\n\\nChat models\\nPrompt templates\\n\\nAnd the LangSmith docs:\\n\\nLangSmith\\nEdit this pageWas this page helpful?PreviousTutorialsNextBuild a ChatbotSetupJupyter NotebookInstallationLangSmithUsing Language ModelsStreamingPrompt TemplatesConclusionCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"]},"metadata":{},"execution_count":11}],"source":["document=loader.load()\n","document"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zRnMnruyyR3N","executionInfo":{"status":"ok","timestamp":1739408504594,"user_tz":-330,"elapsed":459,"user":{"displayName":"Sarvesh Havalagi","userId":"01205626820814641348"}},"outputId":"021d0cfc-f364-4fe4-a8a9-866294b3167b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain'),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add'),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries'),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one\"),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select'),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass'),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and'),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot'),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0'),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a simple LLM'),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a simple LLM application with chat models and prompt templatesOn this pageBuild a simple LLM application with chat models and prompt templates'),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\\nAfter reading this tutorial, you'll have a high level overview of:\"),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"Using language models\\n\\n\\nUsing prompt templates\\n\\n\\nDebugging and tracing your application using LangSmith\\n\\n\\nLet's dive in!\\nSetup‚Äã\\nJupyter Notebook‚Äã\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\\nInstallation‚Äã\\nTo install LangChain run:\"),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='PipCondapip install langchainconda install langchain -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith‚Äã\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nUsing Language Models‚Äã'),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Or, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nUsing Language Models‚Äã\\nFirst up, let\\'s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to supported integrations.'),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Select chat model:Groq‚ñæGroqOpenAIAnthropicAzureGoogle VertexAWSCohereNVIDIAFireworks AIMistral AITogether AIDatabrickspip install -qU \"langchain[groq]\"import getpassimport osif not os.environ.get(\"GROQ_API_KEY\"):  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\\nLet\\'s first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the .invoke method.\\nfrom langchain_core.messages import HumanMessage, SystemMessagemessages = [    SystemMessage(\"Translate the following from English into Italian\"),    HumanMessage(\"hi!\"),]model.invoke(messages)API Reference:HumanMessage | SystemMessage'),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"AIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-32654a56-627c-40e1-a141-ad9350bbfd3e-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\\ntipIf we've enabled LangSmith, we can see that this run is logged to LangSmith, and can see the LangSmith trace. The LangSmith trace reports token usage information, latency, standard model parameters (such as temperature), and other information.\"),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Note that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke(\"Hello\")model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])model.invoke([HumanMessage(\"Hello\")])\\nStreaming‚Äã\\nBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates‚Äã'),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='for token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates‚Äã\\nRight now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\\nPrompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\\nLet\\'s create a prompt template here. It will take in two user variables:'),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='language: The language to translate text into\\ntext: The text to translate'),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='from langchain_core.prompts import ChatPromptTemplatesystem_template = \"Translate the following from English into {language}\"prompt_template = ChatPromptTemplate.from_messages(    [(\"system\", system_template), (\"user\", \"{text}\")])API Reference:ChatPromptTemplate\\nNote that ChatPromptTemplate supports multiple message roles in a single template. We format the language parameter into the system message, and the user text into a user message.\\nThe input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself\\nprompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})prompt\\nChatPromptValue(messages=[SystemMessage(content=\\'Translate the following from English into Italian\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'hi!\\', additional_kwargs={}, response_metadata={})])'),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"We can see that it returns a ChatPromptValue that consists of two messages. If we want to access the messages directly we do:\\nprompt.to_messages()\\n[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]\\nFinally, we can invoke the chat model on the formatted prompt:\\nresponse = model.invoke(prompt)print(response.content)\\nCiao!\\ntipMessage content can contain both text and content blocks with additional structure. See this guide for more information.\\nIf we take a look at the LangSmith trace, we can see exactly what prompt the chat model receives, along with token usage information, latency, standard model parameters (such as temperature), and other information.\\nConclusion‚Äã\"),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"Conclusion‚Äã\\nThat's it! In this tutorial you've learned how to create your first simple LLM application. You've learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith.\\nThis just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we've got a lot of other resources!\\nFor further reading on the core concepts of LangChain, we've got detailed Conceptual Guides.\\nIf you have more specific questions on these concepts, check out the following sections of the how-to guides:\"),\n"," Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Chat models\\nPrompt templates\\n\\nAnd the LangSmith docs:\\n\\nLangSmith\\nEdit this pageWas this page helpful?PreviousTutorialsNextBuild a ChatbotSetupJupyter NotebookInstallationLangSmithUsing Language ModelsStreamingPrompt TemplatesConclusionCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.')]"]},"metadata":{},"execution_count":12}],"source":["# Chunking\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n","documents=text_splitter.split_documents(document)\n","documents"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353,"referenced_widgets":["bac55b888bb0410aaee6fefe39c2b729","e288f20b5d364f78b3cb0d210bb73bd5","d0f6131ab4c945ed940d86804e859b18","4af23be654964a5f933a0d105a71ac31","7b17a571341549df978dafc5a2301a6d","374cdb81fbb54497af6ba63514784899","be4ea1d102d0485fb1d053cdd2c93975","eb9bd562760c4a03b8284366cca538fb","240bdeee1590407a8a1600d2c7726656","2da4f3d8f17c4c5297ff05bf615bcb72","1d41ba7eb0774e17b122387a97a5b517","5686cb6d6c8d49d1bc34365b8960d1dc","5bd531c7eecc4f0cb13ee1a13f7b80be","560def61cf004fc6a552946c8f031674","376a459b9d7049c28951f2aa88a965c2","94bb75047e1945a8853d95b803346398","384cfac971c74828aedfe2622d63bc4f","75e9aa5c60cb42729dd79bd2b0f0ce2b","675d5792915a4e968c18c13d1d963709","1fec199f846f49c4b48df1bbc006488e","8046c7dd60844c848b7b01f52ecc8c2f","8d0cae0181754b0e9175817a9212b8d0","a3e486a6574940f2a4a48cefcbc085b1","d2b0630eb68e4f0fb8b0dcbb19252509","1bcfcce2e96d446e8a3783a8ba75be4a","4d7196b81cba4cb0b171c2db16d4b0aa","b2d4a39ed0724f1b8f01004496464ce4","6ddd969103cd43f69c600aeff7a33b2e","3757aae654394b7bac02f5a8c5223962","703532ce6916444fb26c55c6c4d7235f","c38afb167aaf41f5844dd790fe4a125c","a041642095274ec18e2bd179274d87df","dac87d5972c54615adfc0d828d1a4475","581447df19674bccb054a4ed84255b31","7971e3f9fbc94945b623c256531fa842","1c4123577d4e4e8ab88c32c3f299153f","8966d0f0ee9f492ba1e96c2c4e748f73","08887038770d491d9c5bdae4d8f2e7e2","00cc0a4a437f49bba398d4a6fc756ba3","d3fd40b8b4c04316b015773b390ad6bf","9aec19dfb5b74a39a96a60be748fe795","ec906306a0a24803a26e602acf7e0fd4","808ad10a51d14422a3b30fbed57d5798","395e406e5b5a461f885452b2b1213697","bf4cb95a2e1d4976a3651556f789cd5f","cb45c68d96124ace8be9203b18944bc6","404ad7e8aa0741a69ea2c9147a85265f","78707c6b8ff84f94aa64d1fc20437cc6","bf7d22dfb981434fb20c27afc09e9fbb","45308faed8ff4fad86032daf0a61f7fc","ba792ee3e99849efb7976694c156d635","0466962ca7cb496fbc2b4d9957bd0b3d","4fd4dfe542d04ce5a8f03c883371100f","ac61a12c7f544d5e84aa7d7921c5ec8a","b8eef902e04a4e1785e7378a263e3f3a"]},"id":"9diW4PJEyR3N","executionInfo":{"status":"ok","timestamp":1739408584059,"user_tz":-330,"elapsed":61444,"user":{"displayName":"Sarvesh Havalagi","userId":"01205626820814641348"}},"outputId":"7209aae4-6ff3-4391-947a-01c8c639283b"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-13-4d346fae9d58>:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n","  embedding = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={'device': 'cpu'} ) # Adjust if you're using a GPU\n","/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bac55b888bb0410aaee6fefe39c2b729"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5686cb6d6c8d49d1bc34365b8960d1dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3e486a6574940f2a4a48cefcbc085b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"581447df19674bccb054a4ed84255b31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf4cb95a2e1d4976a3651556f789cd5f"}},"metadata":{}}],"source":["from transformers import AutoTokenizer, AutoModel\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","\n","model_name = \"bert-base-uncased\"\n","\n","# Wrap the model into a HuggingFaceEmbeddings object\n","embedding = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={'device': 'cpu'} ) # Adjust if you're using a GPU\n","\n","# Create the FAISS vectorstore\n","vectorstore = FAISS.from_documents(documents, embedding=embedding)\n","\n","#print(\"FAISS vectorstore created successfully.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VV11VHn3yR3N","outputId":"d3802742-5a18-4ebf-c275-5ced4bd8c406"},"outputs":[{"data":{"text/plain":["\"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\\nAfter reading this tutorial, you'll have a high level overview of:\""]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Ask any question from that website page\n","query=\"This is a relatively simple LLM application \"\n","\n","result=vectorstore.similarity_search(query)\n","result[0].page_content"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"2sknV6EKyR3N","executionInfo":{"status":"ok","timestamp":1739408599560,"user_tz":-330,"elapsed":401,"user":{"displayName":"Sarvesh Havalagi","userId":"01205626820814641348"}}},"outputs":[],"source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","prompt=ChatPromptTemplate.from_template(\n","    \"\"\"\n","Answer the following question based only on the provided context:\n","<context>\n","{context}\n","</context>\n","\n","\n","\"\"\"\n",")"]},{"cell_type":"code","source":["vectorstore.similarity_search(\"Note that ChatModels receive message objects as input\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0D8uiJrU8CDe","executionInfo":{"status":"ok","timestamp":1739410498305,"user_tz":-330,"elapsed":418,"user":{"displayName":"Sarvesh Havalagi","userId":"01205626820814641348"}},"outputId":"06867ab2-13e2-484d-ab8c-d89f5c6b9fbd"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(id='b7eb8d49-7a50-4b26-8c68-df08933debbe', metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Note that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke(\"Hello\")model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])model.invoke([HumanMessage(\"Hello\")])\\nStreaming‚Äã\\nBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates‚Äã'),\n"," Document(id='b5092927-44d0-4839-8f2a-9a3f4113e603', metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain'),\n"," Document(id='13e0211a-7568-49c6-8866-33eab3c01944', metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Or, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nUsing Language Models‚Äã\\nFirst up, let\\'s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to supported integrations.'),\n"," Document(id='a8c2bd2e-b3d8-4b4d-8a11-e947b9d61727', metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='for token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates‚Äã\\nRight now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\\nPrompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\\nLet\\'s create a prompt template here. It will take in two user variables:')]"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6znImj6RyR3O","executionInfo":{"status":"ok","timestamp":1739410621508,"user_tz":-330,"elapsed":407,"user":{"displayName":"Sarvesh Havalagi","userId":"01205626820814641348"}},"outputId":"54a85192-dfbb-473a-fbc0-1533f4fcb9b6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n","  context: RunnableLambda(format_docs)\n","}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n","| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"What are everyone's favorite colors? {context}\"), additional_kwargs={})])\n","| ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7e8761f32250>, default_metadata=())\n","| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"]},"metadata":{},"execution_count":23}],"source":["'''create_stuff_documents_chain function is part of LangChain and is used to create a chain that processes a list of documents by \"stuffing\"\n","them into a prompt and passing them to a language model (LLM) for further processing. This is particularly useful when you want to combine\n","multiple documents into a single prompt and generate a response based on the combined content.'''\n","'''The o/p cmg from vectordb which is called as context will be a list of documents format, these are combined into one using create_stuff_documents_chain and\n","passed to prompt and then passed to llm'''\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","document_chain=create_stuff_documents_chain(llm,prompt)\n","document_chain"]},{"cell_type":"code","source":["'''retriever Converts the vector database (vectorstore) into a retriever interface.\n","The retriever helps fetch relevant documents (context) based on query similarity (e.g., FAISS, Weaviate, ChromaDB).\n","create_retrieval_chain(llm, retriever)\n","This integrates the retriever (context) with the LLM, creating a Retrieval-Augmented Generation (RAG) pipeline.\n","When you provide a query, the retriever fetches relevant documents from the vectorstore, and the LLM then\n","generates a response using those documents as context. (refer written notebook)'''"],"metadata":{"id":"Uos4_VcjDWql"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retriever= vectorstore.as_retriever()\n","\n","from langchain.chains import create_retrieval_chain\n","retrival_chain= create_retrieval_chain(llm,retriever)\n","retrival_chain"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F1v5s2xb88ss","executionInfo":{"status":"ok","timestamp":1739411573529,"user_tz":-330,"elapsed":388,"user":{"displayName":"Sarvesh Havalagi","userId":"01205626820814641348"}},"outputId":"fd2ee371-73b7-4a43-a95c-fb887f732097"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RunnableBinding(bound=RunnableAssign(mapper={\n","  context: RunnableBinding(bound=ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7e8761f32250>, default_metadata=()), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n","})\n","| RunnableAssign(mapper={\n","    answer: VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7e8768977d90>, search_kwargs={})\n","  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":[],"metadata":{"id":"QZ9eu0ELFN1f"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"bac55b888bb0410aaee6fefe39c2b729":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e288f20b5d364f78b3cb0d210bb73bd5","IPY_MODEL_d0f6131ab4c945ed940d86804e859b18","IPY_MODEL_4af23be654964a5f933a0d105a71ac31"],"layout":"IPY_MODEL_7b17a571341549df978dafc5a2301a6d"}},"e288f20b5d364f78b3cb0d210bb73bd5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_374cdb81fbb54497af6ba63514784899","placeholder":"​","style":"IPY_MODEL_be4ea1d102d0485fb1d053cdd2c93975","value":"config.json: 100%"}},"d0f6131ab4c945ed940d86804e859b18":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb9bd562760c4a03b8284366cca538fb","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_240bdeee1590407a8a1600d2c7726656","value":570}},"4af23be654964a5f933a0d105a71ac31":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2da4f3d8f17c4c5297ff05bf615bcb72","placeholder":"​","style":"IPY_MODEL_1d41ba7eb0774e17b122387a97a5b517","value":" 570/570 [00:00&lt;00:00, 24.9kB/s]"}},"7b17a571341549df978dafc5a2301a6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"374cdb81fbb54497af6ba63514784899":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be4ea1d102d0485fb1d053cdd2c93975":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb9bd562760c4a03b8284366cca538fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"240bdeee1590407a8a1600d2c7726656":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2da4f3d8f17c4c5297ff05bf615bcb72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d41ba7eb0774e17b122387a97a5b517":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5686cb6d6c8d49d1bc34365b8960d1dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5bd531c7eecc4f0cb13ee1a13f7b80be","IPY_MODEL_560def61cf004fc6a552946c8f031674","IPY_MODEL_376a459b9d7049c28951f2aa88a965c2"],"layout":"IPY_MODEL_94bb75047e1945a8853d95b803346398"}},"5bd531c7eecc4f0cb13ee1a13f7b80be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_384cfac971c74828aedfe2622d63bc4f","placeholder":"​","style":"IPY_MODEL_75e9aa5c60cb42729dd79bd2b0f0ce2b","value":"model.safetensors: 100%"}},"560def61cf004fc6a552946c8f031674":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_675d5792915a4e968c18c13d1d963709","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1fec199f846f49c4b48df1bbc006488e","value":440449768}},"376a459b9d7049c28951f2aa88a965c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8046c7dd60844c848b7b01f52ecc8c2f","placeholder":"​","style":"IPY_MODEL_8d0cae0181754b0e9175817a9212b8d0","value":" 440M/440M [00:02&lt;00:00, 230MB/s]"}},"94bb75047e1945a8853d95b803346398":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"384cfac971c74828aedfe2622d63bc4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75e9aa5c60cb42729dd79bd2b0f0ce2b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"675d5792915a4e968c18c13d1d963709":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fec199f846f49c4b48df1bbc006488e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8046c7dd60844c848b7b01f52ecc8c2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d0cae0181754b0e9175817a9212b8d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3e486a6574940f2a4a48cefcbc085b1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d2b0630eb68e4f0fb8b0dcbb19252509","IPY_MODEL_1bcfcce2e96d446e8a3783a8ba75be4a","IPY_MODEL_4d7196b81cba4cb0b171c2db16d4b0aa"],"layout":"IPY_MODEL_b2d4a39ed0724f1b8f01004496464ce4"}},"d2b0630eb68e4f0fb8b0dcbb19252509":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ddd969103cd43f69c600aeff7a33b2e","placeholder":"​","style":"IPY_MODEL_3757aae654394b7bac02f5a8c5223962","value":"tokenizer_config.json: 100%"}},"1bcfcce2e96d446e8a3783a8ba75be4a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_703532ce6916444fb26c55c6c4d7235f","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c38afb167aaf41f5844dd790fe4a125c","value":48}},"4d7196b81cba4cb0b171c2db16d4b0aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a041642095274ec18e2bd179274d87df","placeholder":"​","style":"IPY_MODEL_dac87d5972c54615adfc0d828d1a4475","value":" 48.0/48.0 [00:00&lt;00:00, 4.01kB/s]"}},"b2d4a39ed0724f1b8f01004496464ce4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ddd969103cd43f69c600aeff7a33b2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3757aae654394b7bac02f5a8c5223962":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"703532ce6916444fb26c55c6c4d7235f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c38afb167aaf41f5844dd790fe4a125c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a041642095274ec18e2bd179274d87df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dac87d5972c54615adfc0d828d1a4475":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"581447df19674bccb054a4ed84255b31":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7971e3f9fbc94945b623c256531fa842","IPY_MODEL_1c4123577d4e4e8ab88c32c3f299153f","IPY_MODEL_8966d0f0ee9f492ba1e96c2c4e748f73"],"layout":"IPY_MODEL_08887038770d491d9c5bdae4d8f2e7e2"}},"7971e3f9fbc94945b623c256531fa842":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00cc0a4a437f49bba398d4a6fc756ba3","placeholder":"​","style":"IPY_MODEL_d3fd40b8b4c04316b015773b390ad6bf","value":"vocab.txt: 100%"}},"1c4123577d4e4e8ab88c32c3f299153f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9aec19dfb5b74a39a96a60be748fe795","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ec906306a0a24803a26e602acf7e0fd4","value":231508}},"8966d0f0ee9f492ba1e96c2c4e748f73":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_808ad10a51d14422a3b30fbed57d5798","placeholder":"​","style":"IPY_MODEL_395e406e5b5a461f885452b2b1213697","value":" 232k/232k [00:00&lt;00:00, 3.10MB/s]"}},"08887038770d491d9c5bdae4d8f2e7e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00cc0a4a437f49bba398d4a6fc756ba3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3fd40b8b4c04316b015773b390ad6bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9aec19dfb5b74a39a96a60be748fe795":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec906306a0a24803a26e602acf7e0fd4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"808ad10a51d14422a3b30fbed57d5798":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"395e406e5b5a461f885452b2b1213697":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf4cb95a2e1d4976a3651556f789cd5f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cb45c68d96124ace8be9203b18944bc6","IPY_MODEL_404ad7e8aa0741a69ea2c9147a85265f","IPY_MODEL_78707c6b8ff84f94aa64d1fc20437cc6"],"layout":"IPY_MODEL_bf7d22dfb981434fb20c27afc09e9fbb"}},"cb45c68d96124ace8be9203b18944bc6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45308faed8ff4fad86032daf0a61f7fc","placeholder":"​","style":"IPY_MODEL_ba792ee3e99849efb7976694c156d635","value":"tokenizer.json: 100%"}},"404ad7e8aa0741a69ea2c9147a85265f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0466962ca7cb496fbc2b4d9957bd0b3d","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4fd4dfe542d04ce5a8f03c883371100f","value":466062}},"78707c6b8ff84f94aa64d1fc20437cc6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac61a12c7f544d5e84aa7d7921c5ec8a","placeholder":"​","style":"IPY_MODEL_b8eef902e04a4e1785e7378a263e3f3a","value":" 466k/466k [00:00&lt;00:00, 7.26MB/s]"}},"bf7d22dfb981434fb20c27afc09e9fbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45308faed8ff4fad86032daf0a61f7fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba792ee3e99849efb7976694c156d635":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0466962ca7cb496fbc2b4d9957bd0b3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fd4dfe542d04ce5a8f03c883371100f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ac61a12c7f544d5e84aa7d7921c5ec8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8eef902e04a4e1785e7378a263e3f3a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}